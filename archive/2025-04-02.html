
<!DOCTYPE HTML>
<html>
<head>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background-color: #f5f5f5;
      margin: 0;
      padding: 20px;
      color: #333;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
      background-color: #fff;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      padding: 24px;
      position: relative;
    }
    .header {
      text-align: center;
      margin-bottom: 32px;
    }
    .header h1 {
      font-size: 32px;
      font-weight: 700;
      color: #2c3e50;
      margin-bottom: 8px;
    }
    .header p {
      font-size: 16px;
      color: #666;
    }
    .paper-block {
      margin-bottom: 24px;
      padding: 16px;
      border-radius: 8px;
      background-color: #f9f9f9;
      border: 1px solid #ddd;
      opacity: 0;
      transform: translateY(20px);
      transition: opacity 0.6s ease, transform 0.6s ease;
    }
    .paper-block.visible {
      opacity: 1;
      transform: translateY(0);
    }
    .paper-title {
      font-size: 24px;
      font-weight: 700;
      color: #2c3e50;
      margin-bottom: 8px;
    }
    .paper-authors {
      font-size: 14px;
      color: #666;
      margin-bottom: 8px;
    }
    .paper-affiliations {
      font-size: 12px;
      color: #888;
      font-style: italic;
      margin-bottom: 8px;
    }
    .paper-tag {
      font-size: 14px;
      color: #333;
      margin-bottom: 8px;
    }
    .paper-score {
      font-size: 14px;
      color: #333;
      margin-bottom: 8px;
    }
    .paper-abstract {
      font-size: 14px;
      color: #444;
      line-height: 1.6;
      margin-bottom: 16px;
    }
    .paper-tldr {
      font-size: 14px;
      color: #444;
      line-height: 1.6;
      margin-bottom: 16px;
    }
    .paper-actions {
      display: flex;
      gap: 8px;
      align-items: center;
    }
    .paper-actions a {
      text-decoration: none;
      font-size: 14px;
      font-weight: 500;
      color: #fff;
      background-color: #3498db;
      padding: 8px 16px;
      border-radius: 4px;
      transition: background-color 0.3s ease;
    }
    .paper-actions a:hover {
      background-color: #2980b9;
    }
    .heart-btn {
      cursor: pointer;
      font-size: 24px;
      filter: grayscale(1) brightness(0.8);
      transition: filter 0.3s ease, transform 0.2s ease;
    }
    .heart-btn:hover {
      filter: grayscale(0.8) brightness(1);
      transform: scale(1.1);
    }
    .heart-btn.active {
      filter: grayscale(0) drop-shadow(0 0 4px rgba(231, 76, 60, 0.5));
    }
    .star-wrapper {
      font-size: 1.3em;
      line-height: 1;
      display: inline-flex;
      align-items: center;
    }
    .half-star {
      display: inline-block;
      width: 0.5em;
      overflow: hidden;
      white-space: nowrap;
      vertical-align: middle;
    }
    .full-star {
      vertical-align: middle;
    }
    .footer {
      text-align: center;
      font-size: 12px;
      color: #888;
      margin-top: 24px;
      padding-top: 16px;
      border-top: 1px solid #ddd;
    }
    .footer a {
      color: #3498db;
      text-decoration: none;
    }
    .footer a:hover {
      text-decoration: underline;
    }
    .back-to-top {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #3498db;
      color: #fff;
      padding: 10px 16px;
      border-radius: 50%;
      text-decoration: none;
      font-size: 18px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      transition: background-color 0.3s ease;
    }
    .back-to-top:hover {
      background-color: #2980b9;
    }
    /* Toast 提示条样式 */
    .toast {
      position: fixed;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      background-color: #333;
      color: #fff;
      padding: 12px 24px;
      border-radius: 4px;
      font-size: 14px;
      opacity: 0;
      visibility: hidden;
      transition: opacity 0.3s ease, visibility 0.3s ease;
      z-index: 1000;
    }
    .toast.visible {
      opacity: 1;
      visibility: visible;
    }
    /* 模态框样式 */
    .modal {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.5);
      justify-content: center;
      align-items: center;
      z-index: 1000;
    }
    .modal.visible {
      display: flex;
    }
    .modal-content {
      background-color: #fff;
      border-radius: 8px;
      width: 90%;
      max-width: 800px;
      height: 80%;
      overflow: hidden;
      position: relative;
    }
    .modal-iframe {
      width: 100%;
      height: 100%;
      border: none;
    }
    .modal-actions {
      position: absolute;
      top: 10px;
      right: 10px;
      display: flex;
      gap: 8px;
    }
    .modal-button {
      background-color: #3498db;
      color: #fff;
      border: none;
      border-radius: 50%;
      width: 30px;
      height: 30px;
      font-size: 16px;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: background-color 0.3s ease;
    }
    .modal-button:hover {
      background-color: #2980b9;
    }
    .modal-button.close {
      background-color: #e74c3c;
    }
    .modal-button.close:hover {
      background-color: #c0392b;
    }
    /* 提示条样式 */
    .browser-prompt {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #3498db;
      color: #fff;
      padding: 12px;
      text-align: center;
      z-index: 1000;
      display: none;
      display: block;
    }
    .browser-prompt a {
      color: #fff;
      text-decoration: underline;
      margin-left: 8px;
    }
    .browser-prompt a:hover {
      color: #f1c40f;
    }
    .date-navigation {
      display: flex;
      gap: 16px;
      justify-content: center;
      margin-bottom: 16px;
    }
    .nav-link {
      text-decoration: none;
      color: #3498db;
      font-weight: 500;
      padding: 8px 16px;
      border-radius: 4px;
      background-color: #f0f0f0;
      transition: all 0.3s ease;
    }
    .nav-link:hover {
      background-color: #3498db;
      color: white;
    }
  </style>
</head>
<body>

<!-- 提示条 -->
<div class="browser-prompt" id="browser-prompt">
  For the best experience, please open this page in your browser.
  <a href="https://xiangsam.github.io/arxiv-daily/">Open in Browser</a>
</div>

<div class="container">
  <div class="header">
    <h1>Daily Research Papers</h1>
    <p>Your daily dose of the latest research papers, curated just for you.</p>
    <div class="date-navigation">
      <a href="archive/2025-04-01.html" class="nav-link">← Previous Day</a>
      <a href="archive/2025-03-31.html" class="nav-link">← 2 Days Ago</a>
    </div>
  </div>

  <div>
    <br>
    <div class="paper-block">
        <div class="paper-title">Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge</div>
        <div class="paper-authors">Agam Shah, Liqin Ye, Sebastian Jaskowski, Wei Xu, Sudheer Chava</div>
        <div class="paper-affiliations">Georgia Institute of Technology</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Financial Knowledge Bias</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Large Language Models (LLMs) are frequently utilized as sources of knowledge
for question-answering. While it is known that LLMs may lack access to
real-time data or newer data produced after the model's cutoff date, it is less
clear how their knowledge spans across historical information. In this study,
we assess the breadth of LLMs' knowledge using financial data of U.S. publicly
traded companies by evaluating more than 197k questions and comparing model
responses to factual data. We further explore the impact of company
characteristics, such as size, retail investment, institutional attention, and
readability of financial filings, on the accuracy of knowledge represented in
LLMs. Our results reveal that LLMs are less informed about past financial
performance, but they display a stronger awareness of larger companies and more
recent information. Interestingly, at the same time, our analysis also reveals
that LLMs are more likely to hallucinate for larger companies, especially for
data from more recent years. We will make the code, prompts, and model outputs
public upon the publication of the work.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Large Language Models (LLMs) exhibit temporal and cross-sectional biases in financial knowledge, performing better with recent data and larger companies but showing higher hallucination rates in these contexts.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00042v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBBeyond+the+Reported+Cutoff:+Where+Large+Language+Models+Fall+Short+on+Financial+Knowledge%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00042v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</div>
        <div class="paper-authors">Jixuan Leng, Chengsong Huang, Langlin Huang, Bill Yuchen Lin, William W. Cohen, ...</div>
        <div class="paper-affiliations">CMU, UIUC, UW, WUSTL</div>
        <div class="paper-tag"><strong>Tag:</strong> Multimodal Reasoning Evaluation</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Existing reasoning evaluation frameworks for Large Language Models (LLMs) and
Large Vision-Language Models (LVLMs) predominantly either assess text-based
reasoning or vision-language understanding capabilities, with limited dynamic
interplay between textual and visual constraints. To address this limitation,
we introduce CrossWordBench, a benchmark designed to evaluate the reasoning
capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a
task requiring multimodal adherence to semantic constraints from text-based
clues and intersectional constraints from visual grid structures.
CrossWordBench leverages a controllable puzzle generation framework that
produces puzzles in multiple formats (text and image) and offers different
evaluation strategies ranging from direct puzzle solving to interactive modes.
Our extensive evaluation of over 20 models reveals that reasoning LLMs
outperform non-reasoning models substantially by effectively leveraging
crossing-letter constraints. We further demonstrate that LVLMs struggle with
the task, showing a strong correlation between their puzzle-solving performance
and grid-parsing accuracy. Our findings offer insights into the limitations of
the reasoning capabilities of current LLMs and LVLMs, and provide an effective
approach for creating multimodal constrained tasks for future evaluations.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> CrossWordBench is a benchmark evaluating LLMs and LVLMs' multimodal reasoning through crossword puzzles, revealing reasoning models outperform non-reasoning ones and LVLMs struggle with visual constraints.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00043v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBCrossWordBench:+Evaluating+the+Reasoning+Capabilities+of+LLMs+and+LVLMs+with+Controllable+Puzzle+Generation%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00043v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Universal Zero-shot Embedding Inversion</div>
        <div class="paper-authors">Collin Zhang, John X. Morris, Vitaly Shmatikov</div>
        <div class="paper-affiliations">Cornell University</div>
        <div class="paper-tag"><strong>Tag:</strong> Text Embedding Security</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Embedding inversion, i.e., reconstructing text given its embedding and
black-box access to the embedding encoder, is a fundamental problem in both NLP
and security. From the NLP perspective, it helps determine how much semantic
information about the input is retained in the embedding. From the security
perspective, it measures how much information is leaked by vector databases and
embedding-based retrieval systems. State-of-the-art methods for embedding
inversion, such as vec2text, have high accuracy but require (a) training a
separate model for each embedding, and (b) a large number of queries to the
corresponding encoder.
  We design, implement, and evaluate ZSInvert, a zero-shot inversion method
based on the recently proposed adversarial decoding technique. ZSInvert is
fast, query-efficient, and can be used for any text embedding without training
an embedding-specific inversion model. We measure the effectiveness of ZSInvert
on several embeddings and demonstrate that it recovers key semantic information
about the corresponding texts.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> ZSInvert is a universal, zero-shot embedding inversion method that efficiently reconstructs text from embeddings without requiring training separate models for each embedding.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00147v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBUniversal+Zero-shot+Embedding+Inversion%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00147v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Does "Reasoning" with Large Language Models Improve Recognizing, Generating, and Reframing Unhelpful Thoughts?</div>
        <div class="paper-authors">Yilin Qi, Dong Won Lee, Cynthia Breazeal, Hae Won Park</div>
        <div class="paper-affiliations">MIT, Harvard University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Reasoning for Cognitive Reframing</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Cognitive Reframing, a core element of Cognitive Behavioral Therapy (CBT),
helps individuals reinterpret negative experiences by finding positive meaning.
Recent advances in Large Language Models (LLMs) have demonstrated improved
performance through reasoning-based strategies. This inspires a promising
direction of leveraging the reasoning capabilities of LLMs to improve CBT and
mental reframing by simulating the process of critical thinking, potentially
enabling more effective recognition, generation, and reframing of cognitive
distortions. In this work, we investigate the role of various reasoning
methods, including pre-trained reasoning LLMs and augmented reasoning
strategies such as CoT and self-consistency in enhancing LLMs' ability to
perform cognitive reframing tasks. We find that augmented reasoning methods,
even when applied to "outdated" LLMs like GPT-3.5, consistently outperform
state-of-the-art pretrained reasoning models on recognizing, generating and
reframing unhelpful thoughts.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Augmented reasoning methods in LLMs, like CoT and self-consistency, outperform pre-trained reasoning models in cognitive reframing tasks for recognizing, generating, and reframing unhelpful thoughts.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00163v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBDoes+%22Reasoning%22+with+Large+Language+Models+Improve+Recognizing%2C+Generating%2C+and+Reframing+Unhelpful+Thoughts?%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00163v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Large Language Models in Numberland: A Quick Test of Their Numerical Reasoning Abilities</div>
        <div class="paper-authors">Roussel Rahman</div>
        <div class="paper-affiliations">Stanford University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Numerical Reasoning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> An essential element of human mathematical reasoning is our number sense --
an abstract understanding of numbers and their relationships -- which allows us
to solve problems involving vast number spaces using limited computational
resources. Mathematical reasoning of Large Language Models (LLMs) is often
tested on high-level problems (such as Olympiad challenges, geometry, word
problems, and puzzles), but their low-level number sense remains less explored.
We introduce "Numberland," a 100-problem test to evaluate the numerical
reasoning abilities of LLM-based agents. The tasks -- basic operations,
advanced calculations (e.g., exponentiation, complex numbers), prime number
checks, and the 24 game -- aim to test elementary skills and their integration
in solving complex and uncertain problems. We evaluated five LLM-based agents:
OpenAI's o1 and o1-mini, Google Gemini, Microsoft Copilot, and Anthropic
Claude. They scored 74-95% on the first three tasks that allow deterministic
steps to solutions. In the 24 game, which needs trial-and-error search,
performance dropped to 10-73%. We tested the top 24 solver (o1 with 73%
accuracy) on 25 harder problems, and its score fell to 27%, confirming search
as a bottleneck. These results, along with the types of mistakes, suggest a
fragile number of LLMs, which is a bit surprising given their prowess in
challenging benchmarks. The limits of LLM numerical reasoning highlight the
scope of simple, targeted tests to evaluate and explain LLM math skills to
ensure safe use.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> The paper evaluates the numerical reasoning abilities of large language models (LLMs) using a 100-problem test called 'Numberland', revealing that while LLMs perform well on deterministic tasks, their performance drops significantly in trial-and-error problems like the 24 game, highlighting a fragility in their numerical reasoning.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00226v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBLarge+Language+Models+in+Numberland:+A+Quick+Test+of+Their+Numerical+Reasoning+Abilities%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00226v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Do Large Language Models Exhibit Spontaneous Rational Deception?</div>
        <div class="paper-authors">Samuel M. Taylor, Benjamin K. Bergen</div>
        <div class="paper-affiliations">UC San Diego</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Deception and Reasoning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Large Language Models (LLMs) are effective at deceiving, when prompted to do
so. But under what conditions do they deceive spontaneously? Models that
demonstrate better performance on reasoning tasks are also better at prompted
deception. Do they also increasingly deceive spontaneously in situations where
it could be considered rational to do so? This study evaluates spontaneous
deception produced by LLMs in a preregistered experimental protocol using tools
from signaling theory. A range of proprietary closed-source and open-source
LLMs are evaluated using modified 2x2 games (in the style of Prisoner's
Dilemma) augmented with a phase in which they can freely communicate to the
other agent using unconstrained language. This setup creates an opportunity to
deceive, in conditions that vary in how useful deception might be to an agent's
rational self-interest. The results indicate that 1) all tested LLMs
spontaneously misrepresent their actions in at least some conditions, 2) they
are generally more likely to do so in situations in which deception would
benefit them, and 3) models exhibiting better reasoning capacity overall tend
to deceive at higher rates. Taken together, these results suggest a tradeoff
between LLM reasoning capability and honesty. They also provide evidence of
reasoning-like behavior in LLMs from a novel experimental configuration.
Finally, they reveal certain contextual factors that affect whether LLMs will
deceive or not. We discuss consequences for autonomous, human-facing systems
driven by LLMs both now and as their reasoning capabilities continue to
improve.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Large Language Models (LLMs) exhibit spontaneous deception in contexts where it benefits them, with more capable models showing higher deception rates, suggesting a tradeoff between reasoning ability and honesty.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00285v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBDo+Large+Language+Models+Exhibit+Spontaneous+Rational+Deception?%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00285v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Do Chinese models speak Chinese languages?</div>
        <div class="paper-authors">Andrea W Wen-Yi, Unso Eun Seo Jo, David Mimno</div>
        <div class="paper-affiliations">Cornell University</div>
        <div class="paper-tag"><strong>Tag:</strong> Multilingual LLM Performance Evaluation</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> The release of top-performing open-weight LLMs has cemented China's role as a
leading force in AI development. Do these models support languages spoken in
China? Or do they speak the same languages as Western models? Comparing
multilingual capabilities is important for two reasons. First, language ability
provides insights into pre-training data curation, and thus into resource
allocation and development priorities. Second, China has a long history of
explicit language policy, varying between inclusivity of minority languages and
a Mandarin-first policy. To test whether Chinese LLMs today reflect an agenda
about China's languages, we test performance of Chinese and Western open-source
LLMs on Asian regional and Chinese minority languages. Our experiments on
Information Parity and reading comprehension show Chinese models' performance
across these languages correlates strongly (r=0.93) with Western models', with
the sole exception being better Mandarin. Sometimes, Chinese models cannot
identify languages spoken by Chinese minorities such as Kazakh and Uyghur, even
though they are good at French and German. These results provide a window into
current development priorities, suggest options for future development, and
indicate guidance for end users.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Chinese LLMs perform similarly to Western models across most languages but excel in Mandarin, with little support for Chinese minority languages.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00289v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBDo+Chinese+models+speak+Chinese+languages?%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00289v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Hawkeye:Efficient Reasoning with Model Collaboration</div>
        <div class="paper-authors">Jianshu She, Zhuohao Li, Zhemin Huang, Qi Li, Peiran Xu, ...</div>
        <div class="paper-affiliations">Independent Researcher, University of California, Los Angeles, Stanford University, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Reasoning Efficiency</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in
enhancing the reasoning abilities of large language models (LLMs). However, its
efficiency remains a challenge due to the generation of excessive intermediate
reasoning tokens, which introduce semantic redundancy and overly detailed
reasoning steps. Moreover, computational expense and latency are significant
concerns, as the cost scales with the number of output tokens, including those
intermediate steps. In this work, we observe that most CoT tokens are
unnecessary, and retaining only a small portion of them is sufficient for
producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel
post-training and inference framework where a large model produces concise CoT
instructions to guide a smaller model in response generation. HAWKEYE
quantifies redundancy in CoT reasoning and distills high-density information
via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able
to expand responses while reducing token usage and computational cost
significantly. Our evaluation shows that HAWKEYE can achieve comparable
response quality using only 35% of the full CoTs, while improving clarity,
coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can
accelerate end-to-end reasoning by up to 3.4x on complex math tasks while
reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the
models will be available soon.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> HAWKEYE is a framework that improves reasoning efficiency in large language models by using concise Chain-of-Thought instructions from a large model to guide a smaller model, reducing token usage and computational costs while maintaining response quality.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00424v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBHawkeye:Efficient+Reasoning+with+Model+Collaboration%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00424v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</div>
        <div class="paper-authors">Kai Yan, Yufei Xu, Zhengyin Du, Xuesong Yao, Zheyu Wang, ...</div>
        <div class="paper-affiliations">University of Illinois Urbana-Champaign, ByteDance Seed</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Reasoning vs Recitation</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> The rapid escalation from elementary school-level to frontier problems of the
difficulty for LLM benchmarks in recent years have weaved a miracle for
researchers that we are only inches away from surpassing human intelligence.
However, is the LLMs' remarkable reasoning ability indeed comes from true
intelligence by human standards, or are they simply reciting solutions
witnessed during training at an Internet level? To study this problem, we
propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's
recitation behavior when asked simple reasoning problems but with conditions
subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,
we found existing cutting-edge LLMs unanimously exhibits extremely severe
recitation behavior; by changing one phrase in the condition, top models such
as OpenAI-o1 and DeepSeek-R1 can suffer $60\%$ performance loss on elementary
school-level arithmetic and reasoning problems. Such findings are a wake-up
call to the LLM community that compels us to re-evaluate the true intelligence
level of cutting-edge LLMs.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> The paper introduces RoR-Bench, a benchmark revealing that cutting-edge LLMs often recite solutions rather than genuinely reason, showing significant performance drops on subtly altered elementary-level problems.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00509v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBRecitation+over+Reasoning:+How+Cutting-Edge+Language+Models+Can+Fail+on+Elementary+School-Level+Reasoning+Problems?%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00509v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources</div>
        <div class="paper-authors">Weizhi Wang, Yu Tian, Linjie Yang, Heng Wang, Xifeng Yan</div>
        <div class="paper-affiliations">ByteDance, UC Santa Barbara, Nvidia Research</div>
        <div class="paper-tag"><strong>Tag:</strong> Multimodal LLM Pre-Training Efficiency</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> The reproduction of state-of-the-art multimodal LLM pre-training faces
barriers at every stage of the pipeline, including high-quality data filtering,
multimodal data mixture strategies, sequence packing techniques, and training
frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter
Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs
using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic
image resolution and multimodal sequence packing to significantly enhance
pre-training efficiency. The training dataset was carefully curated using both
MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based
filtering methods, substantially improving data quality and training
efficiency. The Open-Qwen2VL pre-training is conducted on academic level
8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\% of 1.4T
multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned
Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on
various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,
indicating the remarkable training efficiency of Open-Qwen2VL. We open-source
all aspects of our work, including compute-efficient and data-efficient
training details, data filtering methods, sequence packing scripts,
pre-training data in WebDataset format, FSDP-based training codebase, and both
base and instruction-tuned model checkpoints. We redefine "fully open" for
multimodal LLMs as the complete release of: 1) the training codebase, 2)
detailed data filtering techniques, and 3) all pre-training and supervised
fine-tuning data used to develop the model.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Open-Qwen2VL is a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours, outperforming partially-open state-of-the-art MLLMs on various benchmarks.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00595v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBOpen-Qwen2VL:+Compute-Efficient+Pre-Training+of+Fully-Open+Multimodal+LLMs+on+Academic+Resources%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00595v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</div>
        <div class="paper-authors">Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, ...</div>
        <div class="paper-affiliations">Georgia Institute of Technology, Meta FAIR</div>
        <div class="paper-tag"><strong>Tag:</strong> Multimodal LLMs for Embodied Agents</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Embodied agents operating in real-world environments must interpret ambiguous
and under-specified human instructions. A capable household robot should
recognize ambiguity and ask relevant clarification questions to infer the user
intent accurately, leading to more effective task execution. To study this
problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a
specific object instance given an ambiguous instruction in a home environment.
The agent must strategically ask minimal, yet relevant, clarification questions
to resolve ambiguity while navigating under partial observability. To solve
this problem, we propose a novel approach that fine-tunes multimodal large
language models (MLLMs) as vision-language-action (VLA) policies using online
reinforcement learning (RL) with LLM-generated rewards. Our method eliminates
the need for large-scale human demonstrations or manually engineered rewards
for training such agents. We benchmark against strong zero-shot baselines,
including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results
demonstrate that our RL-finetuned MLLM outperforms all baselines by a
significant margin ($19.1$-$40.3\%$), generalizing well to novel scenes and
tasks. To the best of our knowledge, this is the first demonstration of
adapting MLLMs as VLA agents that can act and ask for help using LLM-generated
rewards with online RL.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> The paper introduces a method for training embodied agents to resolve ambiguous instructions by asking clarification questions using reinforcement learning with LLM-generated rewards, outperforming zero-shot baselines.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00907v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBGrounding+Multimodal+LLMs+to+Embodied+Agents+that+Ask+for+Help+with+Reinforcement+Learning%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00907v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Multi-Token Attention</div>
        <div class="paper-authors">Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar</div>
        <div class="paper-affiliations">FAIR at Meta</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Attention Mechanisms</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Soft attention is a critical mechanism powering LLMs to locate relevant parts
within a given context. However, individual attention weights are determined by
the similarity of only a single query and key token vector. This "single token
attention" bottlenecks the amount of information used in distinguishing a
relevant part from the rest of the context. To address this issue, we propose a
new attention method, Multi-Token Attention (MTA), which allows LLMs to
condition their attention weights on multiple query and key vectors
simultaneously. This is achieved by applying convolution operations over
queries, keys and heads, allowing nearby queries and keys to affect each
other's attention weights for more precise attention. As a result, our method
can locate relevant context using richer, more nuanced information that can
exceed a single vector's capacity. Through extensive evaluations, we
demonstrate that MTA achieves enhanced performance on a range of popular
benchmarks. Notably, it outperforms Transformer baseline models on standard
language modeling tasks, and on tasks that require searching for information
within long contexts, where our method's ability to leverage richer information
proves particularly beneficial.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> The paper introduces Multi-Token Attention (MTA), a novel attention mechanism that conditions on multiple query and key vectors simultaneously, improving performance in language modeling and long-context tasks.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00927v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBMulti-Token+Attention%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00927v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Taxonomizing Representational Harms using Speech Act Theory</div>
        <div class="paper-authors">Emily Corvi, Hannah Washington, Stefanie Reed, Chad Atalla, Alexandra Chouldechova, ...</div>
        <div class="paper-affiliations">Microsoft Research</div>
        <div class="paper-tag"><strong>Tag:</strong> Generative Language System Fairness</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Representational harms are widely recognized among fairness-related harms
caused by generative language systems. However, their definitions are commonly
under-specified. We present a framework, grounded in speech act theory (Austin,
1962), that conceptualizes representational harms caused by generative language
systems as the perlocutionary effects (i.e., real-world impacts) of particular
types of illocutionary acts (i.e., system behaviors). Building on this argument
and drawing on relevant literature from linguistic anthropology and
sociolinguistics, we provide new definitions stereotyping, demeaning, and
erasure. We then use our framework to develop a granular taxonomy of
illocutionary acts that cause representational harms, going beyond the
high-level taxonomies presented in previous work. We also discuss the ways that
our framework and taxonomy can support the development of valid measurement
instruments. Finally, we demonstrate the utility of our framework and taxonomy
via a case study that engages with recent conceptual debates about what
constitutes a representational harm and how such harms should be measured.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> The paper proposes a framework grounded in speech act theory to taxonomize representational harms in generative language systems, distinguishing between system behaviors and their impacts to improve measurement and mitigation.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00928v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBTaxonomizing+Representational+Harms+using+Speech+Act+Theory%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00928v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning</div>
        <div class="paper-authors">Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, ...</div>
        <div class="paper-affiliations">Mila, University of California Los Angeles, TU Darmstadt, Google DeepMind</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Reasoning Optimization</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Scaling test-time compute has emerged as a key strategy for enhancing the
reasoning capabilities of large language models (LLMs), particularly in tasks
like mathematical problem-solving. A traditional approach, Self-Consistency
(SC), generates multiple solutions to a problem and selects the most common
answer via majority voting. Another common method involves scoring each
solution with a reward model (verifier) and choosing the best one. Recent
advancements in Generative Reward Models (GenRM) reframe verification as a
next-token prediction task, enabling inference-time scaling along a new axis.
Specifically, GenRM generates multiple verification chains-of-thought to score
each solution. Under a limited inference budget, this introduces a fundamental
trade-off: should you spend the budget on scaling solutions via SC or generate
fewer solutions and allocate compute to verification via GenRM? To address
this, we evaluate GenRM against SC under a fixed inference budget.
Interestingly, we find that SC is more compute-efficient than GenRM for most
practical inference budgets across diverse models and datasets. For instance,
GenRM first matches SC after consuming up to 8x the inference compute and
requires significantly more compute to outperform it. Furthermore, we derive
inference scaling laws for the GenRM paradigm, revealing that compute-optimal
inference favors scaling solution generation more aggressively than scaling the
number of verifications. Our work provides practical guidance on optimizing
test-time scaling by balancing solution generation and verification. The code
is available at https://github.com/nishadsinghi/sc-genrm-scaling.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Self-Consistency (SC) is more compute-efficient than Generative Reward Models (GenRM) at lower inference budgets, while GenRM outperforms SC at higher budgets, with optimal performance achieved by scaling solutions more aggressively than verifications.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.01005v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBWhen+To+Solve%2C+When+To+Verify:+Compute-Optimal+Problem+Solving+and+Generative+Verification+for+LLM+Reasoning%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.01005v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization</div>
        <div class="paper-authors">Di Wu, Jia-Chen Gu, Kai-Wei Chang, Nanyun Peng</div>
        <div class="paper-affiliations">University of California, Los Angeles</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Knowledge Verbalization</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Selective retrieval improves retrieval-augmented generation (RAG) by reducing
distractions from low-quality retrievals and improving efficiency. However,
existing approaches under-utilize the inherent knowledge of large language
models (LLMs), leading to suboptimal retrieval decisions and degraded
generation performance. To bridge this gap, we propose Self-Routing RAG
(SR-RAG), a novel framework that binds selective retrieval with knowledge
verbalization. SR-RAG enables an LLM to dynamically decide between external
retrieval and verbalizing its own parametric knowledge. To this end, we design
a multi-task objective that jointly optimizes an LLM on knowledge source
selection, knowledge verbalization, and response generation. We further
introduce dynamic knowledge source inference via nearest neighbor search to
improve the accuracy of knowledge source decision under domain shifts.
Fine-tuning three LLMs with SR-RAG significantly improves both their response
accuracy and inference latency. Compared to the strongest selective retrieval
baseline, SR-RAG reduces retrievals by 29% while improving the performance by
5.1%.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> SR-RAG is a novel RAG framework that integrates selective retrieval with knowledge verbalization, enabling LLMs to dynamically choose between external retrieval and internal knowledge verbalization, improving accuracy and efficiency.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.01018v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBSelf-Routing+RAG:+Binding+Selective+Retrieval+with+Knowledge+Verbalization%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.01018v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier</div>
        <div class="paper-authors">Craig W. Schmidt, Varshini Reddy, Chris Tanner, Yuval Pinter</div>
        <div class="paper-affiliations">Ben-Gurion University of the Negev, Kensho Technologies, MIT</div>
        <div class="paper-tag"><strong>Tag:</strong> Tokenization Algorithms in NLP</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Pre-tokenization, the initial step in many modern tokenization pipelines,
segments text into smaller units called pretokens, typically splitting on
whitespace and punctuation. While this process encourages having full,
individual words as tokens, it introduces a fundamental limitation in most
tokenization algorithms such as Byte Pair Encoding (BPE). Specifically,
pre-tokenization causes the distribution of tokens in a corpus to heavily skew
towards common, full-length words. This skewed distribution limits the benefits
of expanding to larger vocabularies, since the additional tokens appear with
progressively lower counts. To overcome this barrier, we propose BoundlessBPE,
a modified BPE algorithm that relaxes the pretoken boundary constraint. Our
approach selectively merges two complete pretokens into a larger unit we term a
superword. Superwords are not necessarily semantically cohesive. For example,
the pretokens " of" and " the" might be combined to form the superword " of
the". This merging strategy results in a substantially more uniform
distribution of tokens across a corpus than standard BPE, and compresses text
more effectively, with an approximate 20% increase in bytes per token.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> BoundlessBPE modifies Byte Pair Encoding by merging adjacent pretokens into superwords, resulting in a more uniform token distribution and improved text compression.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00178v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBBoundless+Byte+Pair+Encoding:+Breaking+the+Pre-tokenization+Barrier%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00178v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Rack Position Optimization in Large-Scale Heterogeneous Data Centers</div>
        <div class="paper-authors">Chang-Lin Chen, Jiayu Chen, Tian Lan, Zhaoxia Zhao, Hongbo Dong, ...</div>
        <div class="paper-affiliations">Purdue University, George Washington University, Meta Platforms, Inc.</div>
        <div class="paper-tag"><strong>Tag:</strong> Data Center Resource Optimization</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> As rapidly growing AI computational demands accelerate the need for new
hardware installation and maintenance, this work explores optimal data center
resource management by balancing operational efficiency with fault tolerance
through strategic rack positioning considering diverse resources and locations.
Traditional mixed-integer programming (MIP) approaches often struggle with
scalability, while heuristic methods may result in significant sub-optimality.
To address these issues, this paper presents a novel two-tier optimization
framework using a high-level deep reinforcement learning (DRL) model to guide a
low-level gradient-based heuristic for local search. The high-level DRL agent
employs Leader Reward for optimal rack type ordering, and the low-level
heuristic efficiently maps racks to positions, minimizing movement counts and
ensuring fault-tolerant resource distribution. This approach allows scalability
to over 100,000 positions and 100 rack types. Our method outperformed the
gradient-based heuristic by 7\% on average and the MIP solver by over 30\% in
objective value. It achieved a 100\% success rate versus MIP's 97.5\% (within a
20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes
(i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which
showed performance variability under time constraints and high penalties, our
algorithm consistently delivered stable, efficient results - an essential
feature for large-scale data center management.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> This paper presents a two-tier optimization framework using deep reinforcement learning and gradient-based heuristics for efficient and fault-tolerant rack positioning in large-scale heterogeneous data centers, achieving significant performance improvements over traditional methods.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00277v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBRack+Position+Optimization+in+Large-Scale+Heterogeneous+Data+Centers%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00277v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning</div>
        <div class="paper-authors">Ruoxi Xu, Yunjie Ji, Boxi Cao, Yaojie Lu, Hongyu Lin, ...</div>
        <div class="paper-affiliations">University of Chinese Academy of Sciences, a-m-team, Chinese Academy of Sciences</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Knowledge Injection Methods</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Although large language models (LLMs) excel in knowledge recall and
reasoning, their static nature leads to outdated information as the real world
evolves or when adapting to domain-specific knowledge, highlighting the need
for effective knowledge injection. However, current research on knowledge
injection remains superficial, mainly focusing on knowledge memorization and
retrieval. This paper proposes a four-tier knowledge injection framework that
systematically defines the levels of knowledge injection: memorization,
retrieval, reasoning, and association. Based on this framework, we introduce
DeepKnowledge, a synthetic experimental testbed designed for fine-grained
evaluation of the depth of knowledge injection across three knowledge types
(novel, incremental, and updated). We then explore various knowledge injection
scenarios and evaluate the depth of knowledge injection for each scenario on
the benchmark. Experimental results reveal key factors to reach each level of
knowledge injection for LLMs and establish a mapping between the levels of
knowledge injection and the corresponding suitable injection methods, aiming to
provide a comprehensive approach for efficient knowledge injection across
various levels.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> This paper introduces a four-layer knowledge injection framework (memorization, retrieval, reasoning, association) and DeepKnowledge, a testbed for evaluating knowledge injection depth in LLMs, identifying key factors for effective injection at each level.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00472v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBMemorizing+is+Not+Enough:+Deep+Knowledge+Injection+Through+Reasoning%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00472v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Training a Utility-based Retriever Through Shared Context Attribution for Retrieval-Augmented Language Models</div>
        <div class="paper-authors">Yilong Xu, Jinhua Gao, Xiaoming Yu, Yuanhai Xue, Baolong Bi, ...</div>
        <div class="paper-affiliations">University of Chinese Academy of Sciences, Institute of Computing Technology, CAS, Chinese Academy of Sciences</div>
        <div class="paper-tag"><strong>Tag:</strong> Utility-based Retrieval in RALMs</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Retrieval-Augmented Language Models boost task performance, owing to the
retriever that provides external knowledge. Although crucial, the retriever
primarily focuses on semantics relevance, which may not always be effective for
generation. Thus, utility-based retrieval has emerged as a promising topic,
prioritizing passages that provides valid benefits for downstream tasks.
However, due to insufficient understanding, capturing passage utility
accurately remains unexplored. This work proposes SCARLet, a framework for
training utility-based retrievers in RALMs, which incorporates two key factors,
multi-task generalization and inter-passage interaction. First, SCARLet
constructs shared context on which training data for various tasks is
synthesized. This mitigates semantic bias from context differences, allowing
retrievers to focus on learning task-specific utility for better task
generalization. Next, SCARLet uses a perturbation-based attribution method to
estimate passage-level utility for shared context, which reflects interactions
between passages and provides more accurate feedback. We evaluate our approach
on ten datasets across various tasks, both in-domain and out-of-domain, showing
that retrievers trained by SCARLet consistently improve the overall performance
of RALMs.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> The paper introduces SCARLet, a framework for training utility-based retrievers in Retrieval-Augmented Language Models (RALMs) by focusing on multi-task generalization and inter-passage interaction to improve downstream task performance.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00573v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBTraining+a+Utility-based+Retriever+Through+Shared+Context+Attribution+for+Retrieval-Augmented+Language+Models%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00573v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism</div>
        <div class="paper-authors">Dengchun Li, Naizheng Wang, Zihao Zhang, Haoyang Yin, Lei Duan, ...</div>
        <div class="paper-affiliations">Sichuan University, Chinese Academy of Sciences</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Parameter-Efficient Fine-Tuning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Instruction-based fine-tuning of large language models (LLMs) has achieved
remarkable success in various natural language processing (NLP) tasks.
Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts
(MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the
versatility of Mixture of Experts (MoE) models, demonstrating significant
potential for handling multiple downstream tasks. However, the existing routing
mechanisms for MoLE often involve a trade-off between computational efficiency
and predictive accuracy, and they fail to fully address the diverse expert
selection demands across different transformer layers. In this work, we propose
DynMoLE, a hybrid routing strategy that dynamically adjusts expert selection
based on the Tsallis entropy of the router's probability distribution. This
approach mitigates router uncertainty, enhances stability, and promotes more
equitable expert participation, leading to faster convergence and improved
model performance. Additionally, we introduce an auxiliary loss based on
Tsallis entropy to further guide the model toward convergence with reduced
uncertainty, thereby improving training stability and performance. Our
extensive experiments on commonsense reasoning benchmarks demonstrate that
DynMoLE achieves substantial performance improvements, outperforming LoRA by
9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also
conduct a comprehensive ablation study to evaluate the contributions of
DynMoLE's key components.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> DynMoLE introduces a hybrid routing strategy using Tsallis entropy to dynamically adjust expert selection in Mixture of LoRA Experts, improving performance and training stability.</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.00661v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBDynMoLE:+Boosting+Mixture+of+LoRA+Experts+Fine-Tuning+with+a+Hybrid+Routing+Mechanism%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.00661v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=true')">Kimi</a>
            
            <span class="heart-btn" onclick="toggleHeart(this)">❤️</span>
        </div>
    </div>
    </br>
  </div>

  <div class="footer">
    <p>To unsubscribe, remove your email in your Github Action setting.</p>
    <p>To have a full reading experience, <a href='https://xiangsam.github.io/arxiv-daily/'>visit this page</a> in a modern brower.</p>
    <p>&copy; 2023 Research Digest. All rights reserved.</p>
  </div>
</div>

<!-- Toast 提示条 -->
<div class="toast" id="toast">Added to favorites!</div>

<!-- 模态框 -->
<div class="modal" id="modal">
  <div class="modal-content">
    <div class="modal-actions">
      <button class="modal-button" onclick="openInNewTab()" title="Open in new tab">
        <i class="fas fa-external-link-alt"></i>
      </button>
      <button class="modal-button close" onclick="closeModal()" title="Close">
        <i class="fas fa-times"></i>
      </button>
    </div>
    <iframe class="modal-iframe" id="modal-iframe" src=""></iframe>
  </div>
</div>

<a href="#" class="back-to-top">↑</a>

<script>
  // 检测是否在浏览器中正常加载
  function isNormalBrowserLoad() {
    // 检查是否可以执行JavaScript
    if (typeof window!== 'undefined' && 'location' in window) {
      return true;
    }
    return false;
  }

  // 隐藏提示条
  if (isNormalBrowserLoad()) {
    const prompt = document.getElementById('browser-prompt');
    prompt.style.display = 'none'; // 隐藏提示条
  }
  
  // 动态加载效果
  document.addEventListener('DOMContentLoaded', function () {
    const paperBlocks = document.querySelectorAll('.paper-block');
    paperBlocks.forEach((block, index) => {
      setTimeout(() => {
        block.classList.add('visible');
      }, index * 200);
    });
  });

  // 回到顶部按钮
  const backToTopButton = document.querySelector('.back-to-top');
  backToTopButton.addEventListener('click', (e) => {
    e.preventDefault();
    window.scrollTo({ top: 0, behavior: 'smooth' });
  });

  // 心形按钮点击提示
  function toggleHeart(button) {
    button.classList.toggle('active');
    showToast('Added to favorites!');
  }

  // 显示 Toast 提示
  function showToast(message) {
    const toast = document.getElementById('toast');
    toast.textContent = message;
    toast.classList.add('visible');
    setTimeout(() => {
      toast.classList.remove('visible');
    }, 3000); // 3 秒后消失
  }
  
  // 打开模态框
  function openModal(url) {
    const modal = document.getElementById('modal');
    const iframe = document.getElementById('modal-iframe');
    iframe.src = url;
    modal.classList.add('visible');
  }

  // 关闭模态框
  function closeModal() {
    const modal = document.getElementById('modal');
    const iframe = document.getElementById('modal-iframe');
    iframe.src = ''; // 清空 iframe 内容
    modal.classList.remove('visible');
  }

  // 在新标签页打开
  function openInNewTab() {
    const iframe = document.getElementById('modal-iframe');
    const url = iframe.src;
    if (url) {
      window.open(url, '_blank');
    }
  }
</script>

</body>
</html>
