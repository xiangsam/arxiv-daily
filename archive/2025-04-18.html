
<!DOCTYPE HTML>
<html>
<head>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background-color: #f5f5f5;
      margin: 0;
      padding: 20px;
      color: #333;
      background-image: linear-gradient(120deg, #f5f7fa 0%, #c3cfe2 100%);
      min-height: 100vh;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
      background-color: rgba(255, 255, 255, 0.85);
      backdrop-filter: blur(10px);
      -webkit-backdrop-filter: blur(10px);
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(31, 38, 135, 0.15);
      padding: 24px;
      position: relative;
      border: 1px solid rgba(255, 255, 255, 0.18);
    }
    .header {
      text-align: center;
      margin-bottom: 32px;
    }
    .header h1 {
      font-size: 32px;
      font-weight: 700;
      color: #2c3e50;
      margin-bottom: 8px;
    }
    .header p {
      font-size: 16px;
      color: #666;
    }
    .paper-block {
      margin-bottom: 24px;
      padding: 16px;
      border-radius: 8px;
      background-color: rgba(249, 249, 249, 0.9);
      border: 1px solid #ddd;
      opacity: 0;
      transform: translateY(20px);
      transition: all 0.6s cubic-bezier(0.165, 0.84, 0.44, 1);
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
      position: relative;
    }
    .paper-block:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
    }
    .paper-block.visible {
      opacity: 1;
      transform: translateY(0);
    }
    .paper-block::after {
      content: "";
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border-radius: 8px;
      box-shadow: 0 15px 25px rgba(0, 0, 0, 0.15);
      opacity: 0;
      transition: all 0.6s cubic-bezier(0.165, 0.84, 0.44, 1);
      z-index: -1;
    }
    .paper-block:hover::after {
      opacity: 1;
    }
    .paper-title {
      font-size: 24px;
      font-weight: 700;
      color: #2c3e50;
      margin-bottom: 16px;
      position: relative;
      padding-bottom: 10px;
      transition: color 0.3s ease;
    }
    .paper-title::after {
      content: '';
      position: absolute;
      bottom: 0;
      left: 0;
      width: 50px;
      height: 3px;
      background: linear-gradient(90deg, #3498db, #9b59b6);
      transition: width 0.3s ease;
    }
    .paper-block:hover .paper-title::after {
      width: 100%;
    }
    .paper-block:hover .paper-title {
      color: #3498db;
    }
    .paper-authors {
      font-size: 14px;
      color: #666;
      margin-bottom: 8px;
    }
    .paper-affiliations {
      font-size: 12px;
      color: #888;
      font-style: italic;
      margin-bottom: 8px;
    }
    .paper-tag {
      font-size: 14px;
      color: #333;
      margin-bottom: 8px;
    }
    .paper-score {
      font-size: 14px;
      color: #333;
      margin-bottom: 8px;
    }
    .paper-abstract {
      font-size: 14px;
      color: #444;
      line-height: 1.6;
      margin-bottom: 16px;
    }
    .paper-tldr {
      font-size: 14px;
      color: #444;
      line-height: 1.6;
      margin-bottom: 16px;
    }
    .paper-actions {
      display: flex;
      gap: 8px;
      align-items: center;
    }
    .paper-actions a {
      text-decoration: none;
      font-size: 14px;
      font-weight: 500;
      color: #fff;
      background-color: #3498db;
      padding: 8px 16px;
      border-radius: 6px;
      transition: all 0.3s ease;
      box-shadow: 0 2px 5px rgba(52, 152, 219, 0.3);
      position: relative;
      overflow: hidden;
    }
    .paper-actions a:hover {
      background-color: #2980b9;
      transform: translateY(-2px);
      box-shadow: 0 5px 15px rgba(52, 152, 219, 0.4);
    }
    .paper-actions a:active {
      transform: translateY(0);
      box-shadow: 0 2px 5px rgba(52, 152, 219, 0.3);
    }
    .paper-actions a::before {
      content: "";
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2), transparent);
      transition: left 0.7s ease;
    }
    .paper-actions a:hover::before {
      left: 100%;
    }
    .star-wrapper {
      font-size: 1.3em;
      line-height: 1;
      display: inline-flex;
      align-items: center;
    }
    .half-star {
      display: inline-block;
      width: 0.5em;
      overflow: hidden;
      white-space: nowrap;
      vertical-align: middle;
    }
    .full-star {
      vertical-align: middle;
    }
    .footer {
      text-align: center;
      font-size: 12px;
      color: #888;
      margin-top: 24px;
      padding-top: 16px;
      border-top: 1px solid #ddd;
    }
    .footer a {
      color: #3498db;
      text-decoration: none;
    }
    .footer a:hover {
      text-decoration: underline;
    }
    .back-to-top {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background-color: #3498db;
      color: #fff;
      width: 50px;
      height: 50px;
      border-radius: 50%;
      text-decoration: none;
      font-size: 24px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
      transition: all 0.3s ease;
      display: flex;
      align-items: center;
      justify-content: center;
      opacity: 0;
      visibility: visible;
    }
    .back-to-top:hover {
      background-color: #2980b9;
      transform: translateY(-5px);
      box-shadow: 0 6px 15px rgba(0, 0, 0, 0.3);
    }
    /* Toast 提示条样式 */
    .toast {
      position: fixed;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      background-color: #333;
      color: #fff;
      padding: 12px 24px;
      border-radius: 4px;
      font-size: 14px;
      opacity: 0;
      visibility: hidden;
      transition: opacity 0.3s ease, visibility 0.3s ease;
      z-index: 1000;
    }
    .toast.visible {
      opacity: 1;
      visibility: visible;
    }
    /* 模态框样式 */
    .modal {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.5);
      justify-content: center;
      align-items: center;
      z-index: 1000;
    }
    .modal.visible {
      display: flex;
    }
    .modal-content {
      background-color: #fff;
      border-radius: 8px;
      width: 90%;
      max-width: 800px;
      height: 80%;
      overflow: hidden;
      position: relative;
    }
    .modal-iframe {
      width: 100%;
      height: 100%;
      border: none;
    }
    .modal-actions {
      position: absolute;
      top: 10px;
      right: 10px;
      display: flex;
      gap: 8px;
    }
    .modal-button {
      background-color: #3498db;
      color: #fff;
      border: none;
      border-radius: 50%;
      width: 30px;
      height: 30px;
      font-size: 16px;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: background-color 0.3s ease;
    }
    .modal-button:hover {
      background-color: #2980b9;
    }
    .modal-button.close {
      background-color: #e74c3c;
    }
    .modal-button.close:hover {
      background-color: #c0392b;
    }
    /* 提示条样式 */
    .browser-prompt {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #3498db;
      color: #fff;
      padding: 12px;
      text-align: center;
      z-index: 1000;
      display: none;
      display: block;
    }
    .browser-prompt a {
      color: #fff;
      text-decoration: underline;
      margin-left: 8px;
    }
    .browser-prompt a:hover {
      color: #f1c40f;
    }
    .date-navigation {
      display: flex;
      gap: 16px;
      justify-content: center;
      margin-bottom: 16px;
    }
    .nav-link {
      text-decoration: none;
      color: #3498db;
      font-weight: 500;
      padding: 8px 16px;
      border-radius: 4px;
      background-color: #f0f0f0;
      transition: all 0.3s ease;
    }
    .nav-link:hover {
      background-color: #3498db;
      color: white;
    }
    .progress-container {
      position: sticky;
      top: 0;
      width: 100%;
      height: 5px;
      background: transparent;
      z-index: 1000;
    }
    .progress-bar {
      height: 5px;
      background: linear-gradient(90deg, #3498db, #9b59b6);
      width: 0%;
      border-radius: 0 2px 2px 0;
    }
  </style>
</head>
<body>

<!-- 提示条 -->
<div class="browser-prompt" id="browser-prompt">
  For the best experience, please open this page in your browser.
  <a href="https://xiangsam.github.io/arxiv-daily/">Open in Browser</a>
</div>

<div class="progress-container">
  <div class="progress-bar" id="progress-bar"></div>
</div>

<div class="container">
  <div class="header">
    <h1>Daily Research Papers</h1>
    <p>Your daily dose of the latest research papers, curated just for you.</p>
    <div class="date-navigation">
      <a href="archive/2025-04-17.html" class="nav-link">← Previous Day</a>
      <a href="archive/2025-04-16.html" class="nav-link">← 2 Days Ago</a>
    </div>
  </div>

  <div>
    <br>
    <div class="paper-block">
        <div class="paper-title">Beyond Text: Characterizing Domain Expert Needs in Document Research</div>
        <div class="paper-authors">Sireesh Gururaja, Nupoor Gandhi, Jeremiah Milbauer, Emma Strubell</div>
        <div class="paper-affiliations">Carnegie Mellon University</div>
        <div class="paper-tag"><strong>Tag:</strong> Document Understanding in NLP</div>
        <div class="paper-score"><strong>Score:</strong> <span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Working with documents is a key part of almost any knowledge work, from
contextualizing research in a literature review to reviewing legal precedent.
Recently, as their capabilities have expanded, primarily text-based NLP systems
have often been billed as able to assist or even automate this kind of work.
But to what extent are these systems able to model these tasks as experts
conceptualize and perform them now? In this study, we interview sixteen domain
experts across two domains to understand their processes of document research,
and compare it to the current state of NLP systems. We find that our
participants processes are idiosyncratic, iterative, and rely extensively on
the social context of a document in addition its content; existing approaches
in NLP and adjacent fields that explicitly center the document as an object,
rather than as merely a container for text, tend to better reflect our
participants' priorities, though they are often less accessible outside their
research communities. We call on the NLP community to more carefully consider
the role of the document in building useful tools that are accessible,
personalizable, iterative, and socially aware.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 研究通过访谈16位领域专家，发现文档研究过程具有个性化、迭代性和社会性，呼吁NLP社区开发更易访问、可个性化、迭代且社会意识强的工具。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12495v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBBeyond+Text:+Characterizing+Domain+Expert+Needs+in+Document+Research%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12495v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents</div>
        <div class="paper-authors">Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, ...</div>
        <div class="paper-affiliations">OpenAI</div>
        <div class="paper-tag"><strong>Tag:</strong> Web Browsing Agents Benchmark</div>
        <div class="paper-score"><strong>Score:</strong> <span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        <div class="paper-abstract"><strong>Abstract:</strong> We present BrowseComp, a simple yet challenging benchmark for measuring the
ability for agents to browse the web. BrowseComp comprises 1,266 questions that
require persistently navigating the internet in search of hard-to-find,
entangled information. Despite the difficulty of the questions, BrowseComp is
simple and easy-to-use, as predicted answers are short and easily verifiable
against reference answers. BrowseComp for browsing agents can be seen as
analogous to how programming competitions are an incomplete but useful
benchmark for coding agents. While BrowseComp sidesteps challenges of a true
user query distribution, like generating long answers or resolving ambiguity,
it measures the important core capability of exercising persistence and
creativity in finding information. BrowseComp can be found at
https://github.com/openai/simple-evals.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> BrowseComp是一个简单但具有挑战性的基准测试，用于评估代理浏览网页的能力，包含1,266个需要持久搜索互联网的问题。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12516v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBBrowseComp:+A+Simple+Yet+Challenging+Benchmark+for+Browsing+Agents%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12516v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization</div>
        <div class="paper-authors">Adithya Pratapa, Teruko Mitamura</div>
        <div class="paper-affiliations">Carnegie Mellon University</div>
        <div class="paper-tag"><strong>Tag:</strong> Retrieval-augmented Generation for Summarization</div>
        <div class="paper-score"><strong>Score:</strong> <span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Recent advances in long-context reasoning abilities of language models led to
interesting applications in large-scale multi-document summarization. However,
prior work has shown that these long-context models are not effective at their
claimed context windows. To this end, retrieval-augmented systems provide an
efficient and effective alternative. However, their performance can be highly
sensitive to the choice of retrieval context length. In this work, we present a
hybrid method that combines retrieval-augmented systems with long-context
windows supported by recent language models. Our method first estimates the
optimal retrieval length as a function of the retriever, summarizer, and
dataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to
generate a pool of silver references. We use these silver references to
estimate the optimal context length for a given RAG system configuration. Our
results on the multi-document summarization task showcase the effectiveness of
our method across model classes and sizes. We compare against length estimates
from strong long-context benchmarks such as RULER and HELMET. Our analysis also
highlights the effectiveness of our estimation method for very long-context LMs
and its generalization to new classes of LMs.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 本文提出了一种结合检索增强系统和长上下文窗口的混合方法，用于多文档摘要任务中估计最优检索长度，并在不同模型大小和类别中展示了其有效性。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12972v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBEstimating+Optimal+Context+Length+for+Hybrid+Retrieval-augmented+Multi-document+Summarization%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12972v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            <a href="javascript:void(0)" onclick="openModal('https://github.com/adithya7/hybrid-rag')" class="paper-actions">Code</a>
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Antidistillation Sampling</div>
        <div class="paper-authors">Yash Savani, Asher Trockman, Zhili Feng, Avi Schwarzschild, Alexander Robey, ...</div>
        <div class="paper-affiliations">Carnegie Mellon University</div>
        <div class="paper-tag"><strong>Tag:</strong> Model Distillation Defense</div>
        <div class="paper-score"><strong>Score:</strong> <span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Frontier models that generate extended reasoning traces inadvertently produce
rich token sequences that can facilitate model distillation. Recognizing this
vulnerability, model owners may seek sampling strategies that limit the
effectiveness of distillation without compromising model performance.
\emph{Antidistillation sampling} provides exactly this capability. By
strategically modifying a model's next-token probability distribution,
antidistillation sampling poisons reasoning traces, rendering them
significantly less effective for distillation while preserving the model's
practical utility. For further details, see https://antidistillation.com.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 抗蒸馏采样是一种通过调整模型的下一个令牌概率分布来毒化推理轨迹，从而在不影响模型性能的情况下降低蒸馏效果的方法。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.13146v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBAntidistillation+Sampling%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.13146v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks</div>
        <div class="paper-authors">Charlotte Siska, Anush Sankaran</div>
        <div class="paper-affiliations">Microsoft</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Jailbreak Detection</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> In the past few years, Language Models (LMs) have shown par-human
capabilities in several domains. Despite their practical applications and
exceeding user consumption, they are susceptible to jailbreaks when malicious
input exploits the LM's weaknesses, causing it to deviate from its intended
behavior. Current defensive strategies either classify the input prompt as
adversarial or prevent LMs from generating harmful outputs. However, it is
challenging to explain the reason behind the malicious nature of the jailbreak,
which results in a wide variety of closed-box approaches. In this research, we
propose and demonstrate that system-prompt attention from Small Language Models
(SLMs) can be used to characterize adversarial prompts, providing a novel,
explainable, and cheaper defense approach called AttentionDefense. Our research
suggests that the attention mechanism is an integral component in understanding
and explaining how LMs respond to malicious input that is not captured in the
semantic meaning of text embeddings. The proposed AttentionDefense is evaluated
against existing jailbreak benchmark datasets. Ablation studies show that
SLM-based AttentionDefense has equivalent or better jailbreak detection
performance compared to text embedding-based classifiers and GPT-4 zero-shot
detectors.To further validate the efficacy of the proposed approach, we
generate a dataset of novel jailbreak variants of the existing benchmark
dataset using a closed-loop LLM-based multi-agent system. We demonstrate that
the proposed AttentionDefense approach performs robustly on this novel
jailbreak dataset while existing approaches suffer in performance.
Additionally, for practical purposes AttentionDefense is an ideal solution as
it has the computation requirements of a small LM but the performance of a LLM
detector.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> AttentionDefense利用小语言模型的系统提示注意力机制，提供了一种可解释且高效的防御方法，用于检测和防止语言模型的越狱攻击。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12321v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBAttentionDefense:+Leveraging+System+Prompt+Attention+for+Explainable+Defense+Against+Novel+Jailbreaks%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12321v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis</div>
        <div class="paper-authors">Shahriar Noroozizadeh, Jeremy C. Weiss</div>
        <div class="paper-affiliations">National Institutes of Health, Carnegie Mellon University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM for Clinical Text Temporal Annotation</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Clinical case reports and discharge summaries may be the most complete and
accurate summarization of patient encounters, yet they are finalized, i.e.,
timestamped after the encounter. Complementary data structured streams become
available sooner but suffer from incompleteness. To train models and algorithms
on more complete and temporally fine-grained data, we construct a pipeline to
phenotype, extract, and annotate time-localized findings within case reports
using large language models. We apply our pipeline to generate an open-access
textual time series corpus for Sepsis-3 comprising 2,139 case reports from the
Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA
and timeline annotations from I2B2/MIMIC-IV and compare the results to
physician-expert annotations. We show high recovery rates of clinical findings
(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and
strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B
Instruct--0.932). Our work characterizes the ability of LLMs to time-localize
clinical findings in text, illustrating the limitations of LLM use for temporal
reconstruction and providing several potential avenues of improvement via
multimodal integration.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 该研究利用大型语言模型从临床病例报告中重建脓毒症时间序列，构建了一个开放访问的文本时间序列语料库，并验证了模型在提取和时间定位临床发现方面的性能。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12326v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBReconstructing+Sepsis+Trajectories+from+Clinical+Case+Reports+using+LLMs:+the+Textual+Time+Series+Corpus+for+Sepsis%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12326v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Word Embeddings Track Social Group Changes Across 70 Years in China</div>
        <div class="paper-authors">Yuxi Ma, Yongqian Peng, Yixin Zhu</div>
        <div class="paper-affiliations">Peking University</div>
        <div class="paper-tag"><strong>Tag:</strong> Diachronic Word Embeddings in Social Science</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Language encodes societal beliefs about social groups through word patterns.
While computational methods like word embeddings enable quantitative analysis
of these patterns, studies have primarily examined gradual shifts in Western
contexts. We present the first large-scale computational analysis of Chinese
state-controlled media (1950-2019) to examine how revolutionary social
transformations are reflected in official linguistic representations of social
groups. Using diachronic word embeddings at multiple temporal resolutions, we
find that Chinese representations differ significantly from Western
counterparts, particularly regarding economic status, ethnicity, and gender.
These representations show distinct evolutionary dynamics: while stereotypes of
ethnicity, age, and body type remain remarkably stable across political
upheavals, representations of gender and economic classes undergo dramatic
shifts tracking historical transformations. This work advances our
understanding of how officially sanctioned discourse encodes social structure
through language while highlighting the importance of non-Western perspectives
in computational social science.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 该研究通过词嵌入技术分析中国70年来的官方媒体语言，揭示了社会群体表征的稳定性与剧变，特别是在性别和经济阶层方面，与西方社会形成鲜明对比。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12327v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBWord+Embeddings+Track+Social+Group+Changes+Across+70+Years+in+China%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12327v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time</div>
        <div class="paper-authors">Wang Yang, Xiang Yue, Vipin Chaudhary, Xiaotian Han</div>
        <div class="paper-affiliations">Case Western Reserve University, Carnegie Mellon University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Reasoning Enhancement</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Recent advances leverage post-training to enhance model reasoning
performance, which typically requires costly training pipelines and still
suffers from inefficient, overly lengthy outputs. We introduce Speculative
Thinking, a training-free framework that enables large reasoning models to
guide smaller ones during inference at the reasoning level, distinct from
speculative decoding, which operates at the token level. Our approach is based
on two observations: (1) reasoning-supportive tokens such as "wait" frequently
appear after structural delimiters like "\n\n", serving as signals for
reflection or continuation; and (2) larger models exhibit stronger control over
reflective behavior, reducing unnecessary backtracking while improving
reasoning quality. By strategically delegating reflective steps to a more
capable model, our method significantly boosts the reasoning accuracy of
reasoning models while shortening their output. With the assistance of the 32B
reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to
89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average
output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%
decrease. Moreover, when applied to a non-reasoning model
(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%
on the same benchmark, achieving a relative improvement of 7.8%.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 本文提出了一种无需训练的推理框架Speculative Thinking，通过在推理阶段让大型模型指导小型模型的选择性推理步骤，显著提升了小型模型的推理准确性和输出效率。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12329v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBSpeculative+Thinking:+Enhancing+Small-Model+Reasoning+with+Large+Model+Guidance+at+Inference+Time%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12329v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Replicating ReLM Results: Validating Large Language Models with ReLM</div>
        <div class="paper-authors">Reece Adamson, Erin Song</div>
        <div class="paper-affiliations">University of Massachusetts Amherst</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Evaluation Methods</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Validating Large Language Models with ReLM explores the application of formal
languages to evaluate and control Large Language Models (LLMs) for
memorization, bias, and zero-shot performance. Current approaches for
evaluating these types behavior are often slow, imprecise, costly, or introduce
biases of their own, but are necessary due to the importance of this behavior
when productionizing LLMs. This project reproduces key results from the
original ReLM paper and expounds on the approach and applications with an
emphasis on the relevance to the field of systems for machine learning.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> ReLM利用形式语言方法高效评估和控制大型语言模型的记忆、偏见和零样本性能。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12357v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBReplicating+ReLM+Results:+Validating+Large+Language+Models+with+ReLM%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12357v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">On Linear Representations and Pretraining Data Frequency in Language Models</div>
        <div class="paper-authors">Jack Merullo, Noah A. Smith, Sarah Wiegreffe, Yanai Elazar</div>
        <div class="paper-affiliations">University of Washington, Allen Institute for AI (Ai2), Brown University</div>
        <div class="paper-tag"><strong>Tag:</strong> LM Linear Representation Analysis</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Pretraining data has a direct impact on the behaviors and quality of language
models (LMs), but we only understand the most basic principles of this
relationship. While most work focuses on pretraining data's effect on
downstream task behavior, we investigate its relationship to LM
representations. Previous work has discovered that, in language models, some
concepts are encoded `linearly' in the representations, but what factors cause
these representations to form? We study the connection between pretraining data
frequency and models' linear representations of factual relations. We find
evidence that the formation of linear representations is strongly connected to
pretraining term frequencies; specifically for subject-relation-object fact
triplets, both subject-object co-occurrence frequency and in-context learning
accuracy for the relation are highly correlated with linear representations.
This is the case across all phases of pretraining. In OLMo-7B and GPT-J, we
discover that a linear representation consistently (but not exclusively) forms
when the subjects and objects within a relation co-occur at least 1k and 2k
times, respectively, regardless of when these occurrences happen during
pretraining. Finally, we train a regression model on measurements of linear
representation quality in fully-trained LMs that can predict how often a term
was seen in pretraining. Our model achieves low error even on inputs from a
different model with a different pretraining dataset, providing a new method
for estimating properties of the otherwise-unknown training data of closed-data
models. We conclude that the strength of linear representations in LMs contains
signal about the models' pretraining corpora that may provide new avenues for
controlling and improving model behavior: particularly, manipulating the
models' training data to meet specific frequency thresholds.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 研究发现语言模型中线性表示的形成与预训练数据中术语的频率密切相关，尤其是当主语和宾语在关系中共同出现达到一定次数时，线性表示会稳定形成。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12459v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBOn+Linear+Representations+and+Pretraining+Data+Frequency+in+Language+Models%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12459v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?</div>
        <div class="paper-authors">Hansi Zeng, Kai Hui, Honglei Zhuang, Zhen Qin, Zhenrui Yue, ...</div>
        <div class="paper-affiliations">Google DeepMind, University of Massachusetts Amherst, University of Illinois Urbana-Champaign</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Fine-tuning Performance Prediction</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> While metrics available during pre-training, such as perplexity, correlate
well with model performance at scaling-laws studies, their predictive
capacities at a fixed model size remain unclear, hindering effective model
selection and development. To address this gap, we formulate the task of
selecting pre-training checkpoints to maximize downstream fine-tuning
performance as a pairwise classification problem: predicting which of two LLMs,
differing in their pre-training, will perform better after supervised
fine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants
with systematically varied pre-training configurations, e.g., objectives or
data, and evaluate them on diverse downstream tasks after SFT. We first conduct
a study and demonstrate that the conventional perplexity is a misleading
indicator. As such, we introduce novel unsupervised and supervised proxy
metrics derived from pre-training that successfully reduce the relative
performance prediction error rate by over 50%. Despite the inherent complexity
of this task, we demonstrate the practical utility of our proposed proxies in
specific scenarios, paving the way for more efficient design of pre-training
schemes optimized for various downstream tasks.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 研究发现传统困惑度指标无法可靠预测相同规模大语言模型在微调后的性能，提出了新的无监督和监督代理指标，显著降低了预测错误率。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12491v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBCan+Pre-training+Indicators+Reliably+Predict+Fine-tuning+Outcomes+of+LLMs?%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12491v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Memorization vs. Reasoning: Updating LLMs with New Knowledge</div>
        <div class="paper-authors">Aochong Oliver Li, Tanya Goyal</div>
        <div class="paper-affiliations">Cornell University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Knowledge Update Methods</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Large language models (LLMs) encode vast amounts of pre-trained knowledge in
their parameters, but updating them as real-world information evolves remains a
challenge. Existing methodologies and benchmarks primarily target entity
substitutions, failing to capture the full breadth of complex real-world
dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an
automatic pipeline for simulating realistic knowledge updates reflected in an
evidence corpora. KUP's evaluation framework includes direct and indirect
probes to both test memorization of updated facts and reasoning over them, for
any update learning methods. Next, we present a lightweight method called
memory conditioned training (MCT), which conditions tokens in the update corpus
on self-generated "memory" tokens during training. Our strategy encourages LLMs
to surface and reason over newly memorized knowledge at inference. Our results
on two strong LLMs show that (1) KUP benchmark is highly challenging, with the
best CPT models achieving $<2\%$ in indirect probing setting (reasoning) and
(2) MCT training significantly outperforms prior continued pre-training (CPT)
baselines, improving direct probing (memorization) results by up to $25.4\%$.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 本文提出了知识更新游乐场（KUP）框架和记忆条件训练（MCT）方法，用于系统研究大型语言模型在持续预训练中更新知识的有效性，并展示了MCT在直接探测任务中的显著优势。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12523v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBMemorization+vs.+Reasoning:+Updating+LLMs+with+New+Knowledge%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12523v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning</div>
        <div class="paper-authors">Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, ...</div>
        <div class="paper-affiliations">Tsinghua University</div>
        <div class="paper-tag"><strong>Tag:</strong> Embodied AI Reinforcement Learning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Humans can perceive and reason about spatial relationships from sequential
visual observations, such as egocentric video streams. However, how pretrained
models acquire such abilities, especially high-level reasoning, remains
unclear. This paper introduces Embodied-R, a collaborative framework combining
large-scale Vision-Language Models (VLMs) for perception and small-scale
Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a
novel reward system considering think-answer logical consistency, the model
achieves slow-thinking capabilities with limited computational resources. After
training on only 5k embodied video samples, Embodied-R with a 3B LM matches
state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on
both in-distribution and out-of-distribution embodied spatial reasoning tasks.
Embodied-R also exhibits emergent thinking patterns such as systematic analysis
and contextual integration. We further explore research questions including
response length, training on VLM, strategies for reward design, and differences
in model generalization after SFT (Supervised Fine-Tuning) and RL training.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> Embodied-R是一个结合大规模视觉语言模型和小规模语言模型的协作框架，通过强化学习激活基础模型在视频模态中的空间推理能力，并在有限计算资源下实现与最先进多模态推理模型相当的性能。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12680v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBEmbodied-R:+Collaborative+Framework+for+Activating+Embodied+Spatial+Reasoning+in+Foundation+Models+via+Reinforcement+Learning%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12680v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">MAIN: Mutual Alignment Is Necessary for instruction tuning</div>
        <div class="paper-authors">Fanyi Yang, Jianfeng Liu, Xin Zhang, Haoyu Liu, Xixin Cao, ...</div>
        <div class="paper-affiliations">Peking University, Microsoft Corporation</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Instruction Tuning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Instruction tuning has enabled large language models (LLMs) to achieve
remarkable performance, but its success heavily depends on the availability of
large-scale, high-quality instruction-response pairs. However, current methods
for scaling up data generation often overlook a crucial aspect: the alignment
between instructions and responses. We hypothesize that high-quality
instruction-response pairs are not defined by the individual quality of each
component, but by the extent of their alignment with each other. To address
this, we propose a Mutual Alignment Framework (MAIN) that ensures coherence
between the instruction and response through mutual constraints. Experiments
demonstrate that models such as LLaMA and Mistral, fine-tuned within this
framework, outperform traditional methods across multiple benchmarks. This
approach underscores the critical role of instruction-response alignment in
enabling scalable and high-quality instruction tuning for LLMs.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 该论文提出了一个相互对齐框架（MAIN），通过迭代优化指令和响应之间的对齐来提升指令调优的质量，实验证明该方法在多个基准测试中优于传统方法。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12913v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBMAIN:+Mutual+Alignment+Is+Necessary+for+instruction+tuning%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12913v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard</div>
        <div class="paper-authors">Varun Rao, Youran Sun, Mahendra Kumar, Tejas Mutneja, Agastya Mukherjee, ...</div>
        <div class="paper-affiliations">University of Maryland, College Park</div>
        <div class="paper-tag"><strong>Tag:</strong> Financial NLP with LLMs</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> This paper investigates the application of large language models (LLMs) to
financial tasks. We fine-tuned foundation models using the Open FinLLM
Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed
techniques including supervised fine-tuning (SFT), direct preference
optimization (DPO), and reinforcement learning (RL) to enhance their financial
capabilities. The fine-tuned models demonstrated substantial performance gains
across a wide range of financial tasks. Moreover, we measured the data scaling
law in the financial domain. Our work demonstrates the potential of large
language models (LLMs) in financial applications.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 该论文研究了如何通过监督微调、直接偏好优化和强化学习等技术增强大型语言模型在金融任务中的表现，并在Open FinLLM Leaderboard上展示了显著性能提升。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.13125v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBLLMs+Meet+Finance:+Fine-Tuning+Foundation+Models+for+the+Open+FinLLM+Leaderboard%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.13125v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Exploring Expert Failures Improves LLM Agent Tuning</div>
        <div class="paper-authors">Li-Cheng Lan, Andrew Bai, Minhao Cheng, Ruochen Wang, Cho-Jui Hsieh, ...</div>
        <div class="paper-affiliations">OpenAI, UCLA, Pennsylvania State University, University of Maryland</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Agent Fine-Tuning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Large Language Models (LLMs) have shown tremendous potential as agents,
excelling at tasks that require multiple rounds of reasoning and interactions.
Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for
finetuning LLMs as agents: it first imitates expert-generated successful
trajectories and further improves agentic skills through iterative fine-tuning
on successful, self-generated trajectories. However, since the expert (e.g.,
GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler
scenarios, many complex subtasks remain unsolved and persistently
out-of-distribution (OOD). Upon investigating these challenging subtasks, we
discovered that previously failed expert trajectories can often provide
valuable guidance, e.g., plans and key actions, that can significantly improve
agent exploration efficiency and acquisition of critical skills. Motivated by
these observations, we propose Exploring Expert Failures (EEF), which
identifies beneficial actions from failed expert trajectories and integrates
them into the training dataset. Potentially harmful actions are meticulously
excluded to prevent contamination of the model learning process. By leveraging
the beneficial actions in expert failures, EEF successfully solves some
previously unsolvable subtasks and improves agent tuning performance.
Remarkably, our approach achieved a 62\% win rate in WebShop, outperforming RFT
(53. 6\%) and GPT-4 (35. 6\%), and to the best of our knowledge, setting a new
state-of-the-art as the first method to surpass a score of 0.81 in WebShop and
exceed 81 in SciWorld.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 通过分析专家失败轨迹中的有益行动，EEF方法显著提升了LLM智能体的调优性能，在WebShop和SciWorld环境中实现了新的最先进水平。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.13145v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBExploring+Expert+Failures+Improves+LLM+Agent+Tuning%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.13145v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Sleep-time Compute: Beyond Inference Scaling at Test-time</div>
        <div class="paper-authors">Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, ...</div>
        <div class="paper-affiliations">Letta, University of California, Berkeley</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Test-time Compute Optimization</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Scaling test-time compute has emerged as a key ingredient for enabling large
language models (LLMs) to solve difficult problems, but comes with high latency
and inference cost. We introduce sleep-time compute, which allows models to
"think" offline about contexts before queries are presented: by anticipating
what queries users might ask and pre-computing useful quantities, we can
significantly reduce the compute requirements at test-time. To demonstrate the
efficacy of our method, we create modified versions of two reasoning tasks -
Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can
reduce the amount of test-time compute needed to achieve the same accuracy by ~
5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time
compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic
and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,
which extends GSM-Symbolic by including multiple related queries per context.
By amortizing sleep-time compute across related queries about the same context
using Multi-Query GSM-Symbolic, we can decrease the average cost per query by
2.5x. We then conduct additional analysis to understand when sleep-time compute
is most effective, finding the predictability of the user query to be well
correlated with the efficacy of sleep-time compute. Finally, we conduct a
case-study of applying sleep-time compute to a realistic agentic SWE task.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 睡眠时间计算通过在查询前预计算有用信息，显著减少测试时的计算需求，提高大型语言模型的效率和准确性。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.13171v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBSleep-time+Compute:+Beyond+Inference+Scaling+at+Test-time%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.13171v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Evaluating the Diversity and Quality of LLM Generated Content</div>
        <div class="paper-authors">Alexander Shypula, Shuo Li, Botong Zhang, Vishakh Padmakumar, Kayo Yin, ...</div>
        <div class="paper-affiliations">UC Berkeley, University of Pennsylvania, New York University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Diversity Metrics</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Recent work suggests that preference-tuning techniques--including
Reinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,
as well as alternatives like DPO--reduce diversity, creating a dilemma given
that such models are widely deployed in applications requiring diverse outputs.
To address this, we introduce a framework for measuring effective semantic
diversity--diversity among outputs that meet quality thresholds--which better
reflects the practical utility of large language models (LLMs). Using
open-ended tasks that require no human intervention, we find counterintuitive
results: although preference-tuned models--especially those trained via
RL--exhibit reduced lexical and syntactic diversity, they produce greater
effective semantic diversity than SFT or base models, not from increasing
diversity among high-quality outputs, but from generating more high-quality
outputs overall. We discover that preference tuning reduces syntactic diversity
while preserving semantic diversity--revealing a distinction between diversity
in form and diversity in content that traditional metrics often overlook. Our
analysis further shows that smaller models are consistently more
parameter-efficient at generating unique content within a fixed sampling
budget, offering insights into the relationship between model scaling and
diversity. These findings have important implications for applications that
require diverse yet high-quality outputs, from creative assistance to synthetic
data generation.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 研究发现，偏好调优的LLM在减少词汇和句法多样性的同时，通过生成更多高质量输出提高了有效语义多样性，揭示了形式多样性与内容多样性的区别。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12522v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBEvaluating+the+Diversity+and+Quality+of+LLM+Generated+Content%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12522v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer</div>
        <div class="paper-authors">Enming Zhang, Liwen Cao, Yanru Wu, Zijie Zhao, Guan Wang, ...</div>
        <div class="paper-affiliations">Hong Kong Polytechnic University, Southeast University, Tsinghua University</div>
        <div class="paper-tag"><strong>Tag:</strong> Multi-source Prompt Ensemble Learning</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Prompt tuning has emerged as a lightweight adaptation strategy for adapting
foundation models to downstream tasks, particularly in resource-constrained
systems. As pre-trained prompts have become valuable intellectual assets,
combining multiple source prompts offers a promising approach to enhance
generalization to new tasks by leveraging complementary knowledge from diverse
sources. However, naive aggregation of these prompts often leads to
representation collapse due to mutual interference, undermining their
collective potential. To address these challenges, we propose HGPrompt, an
adaptive framework for multi-source prompt transfer that learns optimal
ensemble weights by jointly optimizing dual objectives: transferability and
stability. Specifically, we first introduce an information-theoretic metric to
evaluate the transferability of prompt-induced features on the target task,
capturing the intrinsic alignment between the feature representations.
Additionally, we propose a novel Gradient Alignment Regularization to mitigate
gradient conflicts among prompts, enabling stable and coherent knowledge
transfer from multiple sources while suppressing interference. Extensive
experiments on the large-scale VTAB benchmark demonstrate that HGPrompt
achieves state-of-the-art performance, validating its effectiveness in
multi-source prompt transfer.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 该论文提出了HGPrompt框架，通过优化双重目标（可迁移性和稳定性）来学习多源视觉提示的最优集成权重，从而提升基础模型在下游任务中的适应性。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12311v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBLearning+Optimal+Prompt+Ensemble+for+Multi-source+Visual+Prompt+Transfer%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12311v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br><br>
    <div class="paper-block">
        <div class="paper-title">Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles</div>
        <div class="paper-authors">Zihao Xu, Junchen Ding, Yiling Lou, Kun Zhang, Dong Gong, ...</div>
        <div class="paper-affiliations">University of New South Wales, Fudan University, Carnegie Mellon University</div>
        <div class="paper-tag"><strong>Tag:</strong> LLM Logical Reasoning Evaluation</div>
        <div class="paper-score"><strong>Score:</strong> <div class="star-wrapper"><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="full-star">⭐</span><span class="half-star">⭐</span></div></div>
        <div class="paper-abstract"><strong>Abstract:</strong> Large Language Models (LLMs) have achieved significant progress in language
understanding and reasoning. Evaluating and analyzing their logical reasoning
abilities has therefore become essential. However, existing datasets and
benchmarks are often limited to overly simplistic, unnatural, or contextually
constrained examples. In response to the growing demand, we introduce
SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled
benchmark derived from real-world high-quality Reddit posts containing subtle
logical fallacies. Unlike existing datasets and benchmarks, it provides more
detailed annotations of logical fallacies and features more diverse data. To
further scale up the study and address the limitations of manual data
collection and labeling - such as fallacy-type imbalance and labor-intensive
annotation - we introduce SmartyPat, an automated framework powered by logic
programming-based oracles. SmartyPat utilizes Prolog rules to systematically
generate logically fallacious statements, which are then refined into fluent
natural-language sentences by LLMs, ensuring precise fallacy representation.
Extensive evaluation demonstrates that SmartyPat produces fallacies comparable
in subtlety and quality to human-generated content and significantly
outperforms baseline methods. Finally, experiments reveal nuanced insights into
LLM capabilities, highlighting that while excessive reasoning steps hinder
fallacy detection accuracy, structured reasoning enhances fallacy
categorization performance.</div>
        <div class="paper-tldr"><strong>TLDR:</strong> 本文介绍了SmartyPat-Bench，一个基于真实Reddit帖子构建的逻辑谬误基准数据集，并提出了SmartyPat框架，利用Prolog规则自动生成高质量的逻辑谬误语句，用于评估大型语言模型的逻辑推理能力。</div>
        <div class="paper-actions">
            <a href="javascript:void(0)" onclick="openModal('https://arxiv.org/pdf/2504.12312v1')">PDF</a>
            <a href="javascript:void(0)" onclick="openModal('https://kimi.moonshot.cn/_prefill_chat?prefill_prompt=%E8%AF%B7%E4%BD%A0%E9%98%85%E8%AF%BBSocrates+or+Smartypants:+Testing+Logic+Reasoning+Capabilities+of+Large+Language+Models+with+Logic+Programming-based+Test+Oracles%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%EF%BC%8C%E9%93%BE%E6%8E%A5%E6%98%AF+https://arxiv.org/pdf/2504.12312v1+%EF%BC%8C%E5%B9%B6%E5%9B%9E%E7%AD%94%E4%BB%A5%E4%B8%8B%E9%97%AE%E9%A2%98%EF%BC%9A%0A%2A%2AQ1.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F%2A%2A%0A%2A%2AQ2.+%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6%EF%BC%8C%E8%AF%B7%E7%BB%99%E5%87%BA%E5%B9%B6%E6%80%BB%E7%BB%93%E6%96%B9%E6%B3%95%2A%2A%0A%2A%2AQ3.+%E6%9C%AC%E6%96%87%E8%AF%95%E5%9B%BE%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2A%2A%0A%2A%2AQ4.+%E8%BF%99%E7%AF%87%E8%AE%BA%E6%96%87%E6%8F%90%E5%87%BA%E4%BA%86%E4%BB%80%E4%B9%88%E6%96%B0%E7%9A%84%E6%83%B3%E6%B3%95%E3%80%81%E6%96%B9%E6%B3%95%E6%88%96%E6%A8%A1%E5%9E%8B%EF%BC%9F%E4%B8%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%E5%92%8C%E4%BC%98%E5%8A%BF%EF%BC%9F%2A%2A%0A%2A%2AQ5.+%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F%2A%2A%0A%2A%2AQ6.+%E5%AE%9E%E9%AA%8C%E5%92%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E5%90%A6%E5%BE%88%E5%A5%BD%E5%9C%B0%E6%94%AF%E6%8C%81%E4%BA%86%E9%9C%80%E8%A6%81%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE%2A%2A%0A%E5%9B%9E%E7%AD%94%E6%97%B6%E8%AF%B7%E5%85%88%E9%87%8D%E5%A4%8D%E9%97%AE%E9%A2%98%EF%BC%8C%E5%86%8D%E8%BF%9B%E8%A1%8C%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%9E%E7%AD%94%E3%80%82&system_prompt=%E4%BD%A0%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AD%A6%E6%9C%AF%E4%B8%93%E5%AE%B6%EF%BC%8C%E8%AF%B7%E4%BD%A0%E4%BB%94%E7%BB%86%E9%98%85%E8%AF%BB%E5%90%8E%E7%BB%AD%E9%93%BE%E6%8E%A5%E4%B8%AD%E7%9A%84%E8%AE%BA%E6%96%87%EF%BC%8C%E5%B9%B6%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E8%BF%9B%E8%A1%8C%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9B%9E%E7%AD%94%EF%BC%8C%E4%B8%8D%E8%A6%81%E5%87%BA%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%EF%BC%8C%E5%BD%93%E6%B6%89%E5%8F%8A%E5%88%B0%E5%88%86%E7%82%B9%E5%9B%9E%E7%AD%94%E6%97%B6%EF%BC%8C%E9%BC%93%E5%8A%B1%E4%BD%A0%E4%BB%A5markdown%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA%E3%80%82%E5%AF%B9%E4%BA%8E%E5%BC%95%E7%94%A8%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BD%A0%E9%9C%80%E8%A6%81%E5%8F%8A%E6%97%B6%E5%9C%A8%E5%BC%95%E7%94%A8%E5%86%85%E5%AE%B9%E5%90%8E%E7%BB%99%E5%87%BA%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%E3%80%82&send_immediately=true&force_search=false')">Kimi</a>
            
        </div>
    </div>
    </br>
  </div>

  <div class="footer">
    <p>To unsubscribe, remove your email in your Github Action setting.</p>
    <p>To have a full reading experience, <a href='https://xiangsam.github.io/arxiv-daily/'>visit this page</a> in a modern brower.</p>
    <p>&copy; 2023 Research Digest. All rights reserved.</p>
  </div>
</div>

<!-- Toast 提示条 -->
<div class="toast" id="toast">Added to favorites!</div>

<!-- 模态框 -->
<div class="modal" id="modal">
  <div class="modal-content">
    <div class="modal-actions">
      <button class="modal-button" onclick="openInNewTab()" title="Open in new tab">
        <i class="fas fa-external-link-alt"></i>
      </button>
      <button class="modal-button close" onclick="closeModal()" title="Close">
        <i class="fas fa-times"></i>
      </button>
    </div>
    <iframe class="modal-iframe" id="modal-iframe" src=""></iframe>
  </div>
</div>

<a href="#" class="back-to-top">↑</a>

<script>
  // 检测是否在浏览器中正常加载
  function isNormalBrowserLoad() {
    // 检查是否可以执行JavaScript
    if (typeof window!== 'undefined' && 'location' in window) {
      return true;
    }
    return false;
  }

  // 隐藏提示条
  if (isNormalBrowserLoad()) {
    const prompt = document.getElementById('browser-prompt');
    prompt.style.display = 'none'; // 隐藏提示条
  }
  
  // 动态加载效果
  document.addEventListener('DOMContentLoaded', function () {
    const paperBlocks = document.querySelectorAll('.paper-block');
    paperBlocks.forEach((block, index) => {
      setTimeout(() => {
        block.classList.add('visible');
      }, index * 200);
    });
  });

  // 回到顶部按钮
  const backToTopButton = document.querySelector('.back-to-top');
  backToTopButton.addEventListener('click', (e) => {
    e.preventDefault();
    window.scrollTo({ top: 0, behavior: 'smooth' });
  });

  // 显示 Toast 提示
  function showToast(message) {
    const toast = document.getElementById('toast');
    toast.textContent = message;
    toast.classList.add('visible');
    setTimeout(() => {
      toast.classList.remove('visible');
    }, 3000); // 3 秒后消失
  }
  
  // 打开模态框
  function openModal(url) {
    const modal = document.getElementById('modal');
    const iframe = document.getElementById('modal-iframe');
    iframe.src = url;
    modal.classList.add('visible');
  }

  // 关闭模态框
  function closeModal() {
    const modal = document.getElementById('modal');
    const iframe = document.getElementById('modal-iframe');
    iframe.src = ''; // 清空 iframe 内容
    modal.classList.remove('visible');
  }

  // 在新标签页打开
  function openInNewTab() {
    const iframe = document.getElementById('modal-iframe');
    const url = iframe.src;
    if (url) {
      window.open(url, '_blank');
    }
  }
  
  // 滚动进度条
  window.onscroll = function() {scrollFunction()};
  
  function scrollFunction() {
    var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
    var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    var scrolled = (winScroll / height) * 100;
    document.getElementById("progress-bar").style.width = scrolled + "%";
    
    // 显示/隐藏回到顶部按钮
    if (winScroll > 300) {
      backToTopButton.style.opacity = "1";
    } else {
      backToTopButton.style.opacity = "0";
    }
  }
</script>

</body>
</html>
